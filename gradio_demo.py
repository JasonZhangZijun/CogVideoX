import torch
import gradio as gr
from diffusers import AutoencoderKLCogVideoX, CogVideoXImageToVideoPipeline, CogVideoXTransformer3DModel
from diffusers.utils import export_to_video, load_image
from transformers import T5EncoderModel
from PIL import Image
import os

# è®¾ç½®æ¨¡å‹è·¯å¾„å’Œè®¾å¤‡
model_path = "/home/zijun/workspace/CogVideo/checkpoints/CogVideoX-5B-I2V"
device = "cuda:5" if torch.cuda.is_available() else "cpu"
torch.cuda.set_device(0)

# åŠ è½½æ¨¡å‹ç»„ä»¶
transformer = CogVideoXTransformer3DModel.from_pretrained(model_path, subfolder="transformer", torch_dtype=torch.float16).to(device)
text_encoder = T5EncoderModel.from_pretrained(model_path, subfolder="text_encoder", torch_dtype=torch.float16).to(device)
vae = AutoencoderKLCogVideoX.from_pretrained(model_path, subfolder="vae", torch_dtype=torch.float16).to(device)

# åˆå§‹åŒ– I2V pipeline
pipe = CogVideoXImageToVideoPipeline.from_pretrained(
    model_path,
    text_encoder=text_encoder,
    transformer=transformer,
    vae=vae,
    torch_dtype=torch.float16,
).to(device)

# å¯ç”¨ä¼˜åŒ–
pipe.enable_sequential_cpu_offload()

# è®¾å®šè¾“å‡ºç›®å½•
os.makedirs("./output", exist_ok=True)

def infer(prompt: str, image: Image, num_inference_steps: int = 50):
    """ æ‰§è¡Œ I2V æ¨ç† """
    # å¤„ç†è¾“å…¥å›¾ç‰‡
    image = image.convert("RGB")  # ç¡®ä¿æ˜¯ RGB æ ¼å¼
    image.save("./output/input_image.jpg")  # ä¸´æ—¶ä¿å­˜å›¾ç‰‡
    image = load_image("./output/input_image.jpg")

    # ç”Ÿæˆè§†é¢‘
    video = pipe(
        image=image,
        prompt=prompt,
        guidance_scale=6.0,
        use_dynamic_cfg=True,
        num_inference_steps=num_inference_steps
    ).frames[0]

    # å¯¼å‡ºè§†é¢‘
    video_path = "./output/generated_video.mp4"
    export_to_video(video, video_path, fps=8)

    return video_path

# Gradio ç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("# ğŸš€ CogVideoX-5B-I2V Gradio Demo")

    with gr.Row():
        image_input = gr.Image(label="è¾“å…¥å›¾ç‰‡", type="pil")
        prompt = gr.Textbox(label="è¾“å…¥æ–‡æœ¬æè¿°", placeholder="è¾“å…¥ä½ çš„æè¿°...", lines=3)

    num_steps = gr.Slider(10, 100, value=50, step=10, label="æ¨ç†æ­¥æ•°")
    generate_button = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘")
    video_output = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")

    generate_button.click(
        infer,
        inputs=[prompt, image_input, num_steps],
        outputs=[video_output],
    )

if __name__ == "__main__":
    demo.launch()
